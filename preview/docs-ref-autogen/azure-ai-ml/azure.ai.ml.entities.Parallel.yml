### YamlMime:PythonClass
uid: azure.ai.ml.entities.Parallel
name: Parallel
fullName: azure.ai.ml.entities.Parallel
module: azure.ai.ml.entities
inheritances:
- azure.ai.ml.entities._builders.base_node.BaseNode
summary: 'Base class for parallel node, used for parallel component version

  consumption.


  You should not instantiate this class directly. Instead, you should

  create from builder function: parallel.'
constructor:
  syntax: 'Parallel(*, component: ParallelComponent | str, compute: str = None, inputs:
    Dict[str, PipelineInput | NodeOutput | Input | str | bool | int | float | Enum]
    = None, outputs: Dict[str, str | Output] = None, retry_settings: Dict[str, RetrySettings
    | str] = None, logging_level: str = None, max_concurrency_per_instance: int =
    None, error_threshold: int = None, mini_batch_error_threshold: int = None, input_data:
    str = None, task: Dict[str, ParallelTask | str] = None, mini_batch_size: int =
    None, resources: JobResourceConfiguration = None, environment_variables: Dict
    = None, **kwargs)'
  parameters:
  - name: component
    description: Id or instance of the parallel component/job to be run for the step
    isRequired: true
    types:
    - <xref:parallelComponent>
  - name: name
    description: Name of the parallel.
    isRequired: true
    types:
    - <xref:str>
  - name: description
    description: Description of the commad.
    isRequired: true
    types:
    - <xref:str>
  - name: tags
    description: Tag dictionary. Tags can be added, removed, and updated.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: properties
    description: The job property dictionary.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: display_name
    description: Display name of the job.
    isRequired: true
    types:
    - <xref:str>
  - name: retry_settings
    description: parallel job run failed retry
    isRequired: true
    types:
    - <xref:azure.ai.ml.entities.BatchRetrySettings>
  - name: logging_level
    description: A string of the logging level name
    isRequired: true
    types:
    - <xref:str>
  - name: max_concurrency_per_instance
    description: The max parallellism that each compute instance has.
    isRequired: true
    types:
    - <xref:int>
  - name: error_threshold
    description: The number of item processing failures should be ignored.
    isRequired: true
    types:
    - <xref:int>
  - name: mini_batch_error_threshold
    description: The number of mini batch processing failures should be ignored.
    isRequired: true
    types:
    - <xref:int>
  - name: task
    description: The parallel task.
    isRequired: true
    types:
    - <xref:azure.ai.ml.entities.ParallelTask>
  - name: mini_batch_size
    description: 'For FileDataset input, this field is the number of files a user
      script can process

      in one run() call. For TabularDataset input, this field is the approximate size
      of data the user script

      can process in one run() call. Example values are 1024, 1024KB, 10MB, and 1GB.

      (optional, default value is 10 files for FileDataset and 1MB for TabularDataset.)
      This value could be set

      through PipelineParameter'
    isRequired: true
    types:
    - <xref:str>
  - name: input_data
    description: The input data.
    isRequired: true
    types:
    - <xref:str>
  - name: inputs
    description: Inputs of the component/job.
    isRequired: true
    types:
    - <xref:dict>
  - name: outputs
    description: Outputs of the component/job.
    isRequired: true
    types:
    - <xref:dict>
methods:
- uid: azure.ai.ml.entities.Parallel.clear
  name: clear
  signature: clear() -> None.  Remove all items from D.
- uid: azure.ai.ml.entities.Parallel.copy
  name: copy
  signature: copy() -> a shallow copy of D
- uid: azure.ai.ml.entities.Parallel.dump
  name: dump
  summary: Dump the job content into a file in yaml format.
  signature: 'dump(dest: str | PathLike | IO, **kwargs) -> None'
  parameters:
  - name: dest
    description: 'The destination to receive this job''s content.

      Must be either a path to a local file, or an already-open file stream.

      If dest is a file path, a new file will be created,

      and an exception is raised if the file exists.

      If dest is an open file, the file will be written to directly,

      and an exception will be raised if the file is not writable.'
    isRequired: true
    types:
    - <xref:Union>[<xref:PathLike>, <xref:str>, <xref:IO>[<xref:AnyStr>]]
- uid: azure.ai.ml.entities.Parallel.fromkeys
  name: fromkeys
  summary: Create a new dictionary with keys from iterable and values set to value.
  signature: fromkeys(value=None, /)
  parameters:
  - name: type
    isRequired: true
  - name: iterable
    isRequired: true
  - name: value
    defaultValue: None
- uid: azure.ai.ml.entities.Parallel.get
  name: get
  summary: Return the value for key if key is in the dictionary, else default.
  signature: get(key, default=None, /)
  parameters:
  - name: key
    isRequired: true
  - name: default
    defaultValue: None
- uid: azure.ai.ml.entities.Parallel.items
  name: items
  signature: items() -> a set-like object providing a view on D's items
- uid: azure.ai.ml.entities.Parallel.keys
  name: keys
  signature: keys() -> a set-like object providing a view on D's keys
- uid: azure.ai.ml.entities.Parallel.pop
  name: pop
  summary: 'If the key is not found, return the default if given; otherwise,

    raise a KeyError.'
  signature: pop(k, [d]) -> v, remove specified key and return the corresponding value.
- uid: azure.ai.ml.entities.Parallel.popitem
  name: popitem
  summary: 'Remove and return a (key, value) pair as a 2-tuple.


    Pairs are returned in LIFO (last-in, first-out) order.

    Raises KeyError if the dict is empty.'
  signature: popitem()
- uid: azure.ai.ml.entities.Parallel.set_resources
  name: set_resources
  summary: Set resources for Parallel.
  signature: 'set_resources(*, instance_type: str | List[str] = None, instance_count:
    int = None, properties: Dict = None, docker_args: str = None, shm_size: str =
    None, **kwargs)'
- uid: azure.ai.ml.entities.Parallel.setdefault
  name: setdefault
  summary: 'Insert key with a value of default if key is not in the dictionary.


    Return the value for key if key is in the dictionary, else default.'
  signature: setdefault(key, default=None, /)
  parameters:
  - name: key
    isRequired: true
  - name: default
    defaultValue: None
- uid: azure.ai.ml.entities.Parallel.update
  name: update
  summary: 'If E is present and has a .keys() method, then does:  for k in E: D[k]
    = E[k]

    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] =
    v

    In either case, this is followed by: for k in F:  D[k] = F[k]'
  signature: update([E], **F) -> None.  Update D from dict/iterable E and F.
- uid: azure.ai.ml.entities.Parallel.values
  name: values
  signature: values() -> an object providing a view on D's values
attributes:
- uid: azure.ai.ml.entities.Parallel.base_path
  name: base_path
  summary: Base path of the resource.
  return:
    description: Base path of the resource
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.Parallel.component
  name: component
- uid: azure.ai.ml.entities.Parallel.creation_context
  name: creation_context
  summary: Creation context.
  return:
    description: Creation metadata of the resource.
    types:
    - <xref:Optional>[<xref:azure.ai.ml.entities.SystemData>]
- uid: azure.ai.ml.entities.Parallel.id
  name: id
  summary: Resource ID.
  return:
    description: Global id of the resource, Azure Resource Manager ID
    types:
    - <xref:Optional>[<xref:str>]
- uid: azure.ai.ml.entities.Parallel.inputs
  name: inputs
- uid: azure.ai.ml.entities.Parallel.log_files
  name: log_files
  summary: Job output files.
  return:
    description: Dictionary of log names to url.
    types:
    - <xref:Optional>[<xref:Dict>[<xref:str>, <xref:str>]]
- uid: azure.ai.ml.entities.Parallel.outputs
  name: outputs
- uid: azure.ai.ml.entities.Parallel.resources
  name: resources
- uid: azure.ai.ml.entities.Parallel.retry_settings
  name: retry_settings
- uid: azure.ai.ml.entities.Parallel.status
  name: status
  summary: "Status of the job.\n\nCommon values returned include \"Running\", \"Completed\"\
    , and \"Failed\".\n\n> [!NOTE]\n> NotStarted - This is a temporary state client-side\
    \ Run objects are in before cloud submission.\n>\n> \n>\n> Starting - The Run\
    \ has started being processed in the cloud. The caller has a run ID at this point.\n\
    >\n> \n>\n> Provisioning - Returned when on-demand compute is being created for\
    \ a given job submission.\n>\n> \n>\n> Preparing - The run environment is being\
    \ prepared:\n>\n> \n>\n> docker image build\n>\n> \n>\n> conda environment setup\n\
    >\n> \n>\n> Queued - The job is queued in the compute target. For example, in\
    \ BatchAI the job is in queued state\n>\n> \n>\n> while waiting for all the requested\
    \ nodes to be ready.\n>\n> \n>\n> Running - The job started to run in the compute\
    \ target.\n>\n> \n>\n> Finalizing - User code has completed and the run is in\
    \ post-processing stages.\n>\n> \n>\n> CancelRequested - Cancellation has been\
    \ requested for the job.\n>\n> \n>\n> Completed - The run completed successfully.\
    \ This includes both the user code and run\n>\n> \n>\n> post-processing stages.\n\
    >\n> \n>\n> Failed - The run failed. Usually the Error property on a run will\
    \ provide details as to why.\n>\n> \n>\n> Canceled - Follows a cancellation request\
    \ and indicates that the run is now successfully cancelled.\n>\n> \n>\n> NotResponding\
    \ - For runs that have Heartbeats enabled, no heartbeat has been recently sent.\n\
    >"
  return:
    description: Status of the job.
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.Parallel.studio_url
  name: studio_url
  summary: Azure ML studio endpoint.
  return:
    description: URL to the job detail page.
    types:
    - <xref:Optional>[<xref:str>]
- uid: azure.ai.ml.entities.Parallel.task
  name: task
- uid: azure.ai.ml.entities.Parallel.type
  name: type
  summary: Type of the job, supported are 'command' and 'sweep'.
  return:
    description: Type of the job.
    types:
    - <xref:str>
